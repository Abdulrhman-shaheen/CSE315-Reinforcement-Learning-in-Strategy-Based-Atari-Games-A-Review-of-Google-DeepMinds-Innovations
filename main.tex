\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{tabularx}
\usepackage{url}
\usepackage{titlesec}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Reinforcement Learning in Strategy-Based and Atari Games: A Review of Google DeepMind's Innovations\\}

\input{sections/0Authors/authors.tex}

\maketitle
\thispagestyle{plain}
\pagestyle{plain}
\begin{abstract}

    \input{sections/1Abstract/abstract.tex}

\end{abstract}

\begin{IEEEkeywords}
    Deep Reinforcement Learning, Google DeepMind, AlphaGo, AlphaGo Zero, MuZero, Atari Games, Go, Chess, Shogi,
\end{IEEEkeywords}

\section{Introduction}

\input{sections/2Introduction/introdcution.tex}
\section{Related Work}
\input{sections/1.5Related work/relatedwork.tex}

\section{Background}
\input{sections/3Background/background.tex}

\section{AlphaGo}
\input{sections/4AlphaGo/AlphaGo.tex}

\section{AlphaGo Zero}
\input{sections/5AlphaGo Zero/AlphaGoZero.tex}

\section{Alpha Zero}
\input{sections/6AlphaZero/AlphaZero.tex}

\section{MuZero}
\input{sections/7MuZero/MuZero.tex}

\section{Real-World Applications}
\input{sections/8Advancments/advancments.tex}

\section{Future Directions}
\input{sections/9Future Directions/futureDirections.tex}

\section*{Conclusion}
\input{sections/10Conclusion/conclusion.tex}


\begin{thebibliography}{00}
    %-------Introduction-------
    \bibitem{I1}  N. Y. Georgios and T. Julian, Artificial Intelligence and Games. New York: Springer, 2018.
    \bibitem{I2}  N. Justesen, P. Bontrager, J. Togelius, S. Risi, (2019). Deep learning for video game playing. arXiv.
    \bibitem{I3}  V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wierstra, M. Riedmiller, (2013). Playing Atari with deep reinforcement learning. arXiv.
    \bibitem{I4}  A. Graves, G. Wayne, I. Danihelka, (2014). Neural Turing Machines. arXiv.
    \bibitem{I5}  C.J.C.H.Watkins, P. Dayan, Q-learning. Mach Learn 8, 279–292 (1992).
    \bibitem{I6}  DeepMind, (2015, February 12), Deep reinforcement learning.
    \bibitem{I7}  T. Schaul, J. Quan, I. Antonoglou, D. Silver, (2015). Prioritized Experience Replay. arXiv.
    \bibitem{I8}  V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley, D. Silver, K. Kavukcuoglu, (2016). Asynchronous Methods for Deep Reinforcement Learning. arXiv.
    \bibitem{I9}  A. Kailash, P. D. Marc, B. Miles, and A. B. Anil, (2017). Deep Reinforcement Learning: A Brief Survey. IEEE Signal Processing Magazine, vol. 34, pp. 26–38, 2017. arXiv.
    \bibitem{I10} D. Zhao,  K. Shao, Y. Zhu, D. Li, Y. Chen, H. Wang, D. Liu, T. Zhou, and C. Wang, “Review of deep reinforcement learning and discussions on the development of computer Go,” Control Theory and Applications, vol. 33, no. 6, pp. 701–717, 2016 arXiv.
    \bibitem{I11} Z. Tang, K. Shao, D. Zhao, and Y. Zhu, “Recent progress of deep reinforcement learning: from AlphaGo to AlphaGo Zero,” Control Theory and Applications, vol. 34, no. 12, pp. 1529–1546, 2017.
    \bibitem{I12} K. Shao, Z. Tang, Y. Zhu, N. Li, D. Zhao, (2019). A survey of deep reinforcement learning in video games. arXiv.
    
    %-------Background-------
    \bibitem{bg1} L. Thorndike and D. Bruce, Animal Intelligence. Routledge, 2017.
    \bibitem{bg2} R. S. Sutton and A. Barto, Reinforcement learning : an introduction. Cambridge, Ma ; London: The Mit Press, 2018.
    \bibitem{bg3} A. Kumar Shakya, G. Pillai, and S. Chakrabarty, “Reinforcement Learning Algorithms: A brief survey,” Expert Systems with Applications, vol. 231, p. 120495, May 2023
    \bibitem{bg4} Mnih, Volodymyr, et al. “Human-Level Control through Deep Reinforcement Learning.” Nature, vol. 518, no. 7540, Feb. 2015, pp. 529–533.
  %-------AlphaGo-------
    \bibitem{Silver2016} D. Silver et al., “Mastering the game of Go with deep neural networks and tree search,” \textit{Nature}, vol. 529, no. 7587, pp. 484–489, Jan. 2016, doi: https://doi.org/10.1038/nature16961.
    %-------AlphaGoZero-------
    \bibitem{agz1} Silver, David, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, et al. 2017. “Mastering the Game of Go without Human Knowledge.” Nature 550 (7676): 354–59. https://doi.org/10.1038/nature24270.

    % ---------------- AlphaZero --------------
    \bibitem{az1} David Silver et al. ,A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play. Science 362,1140-1144(2018). doi:10.1126/science.aar6404

    %-------Muzero-------
    \bibitem{mz1} J. Schrittwieser et al., “Mastering Atari, go, chess and shogi by planning with a learned model,” Nature, vol. 588, no. 7839, pp. 604–609, Dec. 2020. doi:10.1038/s41586-020-03051-4 
    %-------Advancememts-------
    \bibitem{fawzi2022discovering}
    A. Fawzi et al., ``Discovering faster matrix multiplication algorithms with reinforcement learning,'' \textit{Nature}, vol. 610, no. 7930, pp. 47--53, Oct. 2022, doi: 10.1038/s41586-022-05172-4.
    
    \bibitem{alphadev2023}
    D. J. Mankowitz et al., ``Faster sorting algorithms discovered using deep reinforcement learning,'' \textit{Nature}, vol. 618, no. 7964, pp. 257–263, Jun. 2023, doi: 10.1038/s41586-023-06004-9.
    
    \bibitem{muzero_real_world_2022}
    DeepMind, ``MuZero's first step from research into the real world,'' DeepMind, Feb. 11, 2022. [Online]. Available: \url{https://www.deepmind.com/blog/muzeros-first-step-from-research-into-the-real-world}. [Accessed: Oct. 1, 2023].
    
    \bibitem{jumper2021highly}
    J. Jumper et al., ``Highly accurate protein structure prediction with AlphaFold,'' \textit{Nature}, vol. 596, no. 7873, pp. 583–589, Aug. 2021, doi: 10.1038/s41586-021-03819-2.

    
    %-------Future Directions-------
    \bibitem{wang2023model} X. Wang, Z. Zhang, W. Zhang, ``Model-based Multi-agent Reinforcement Learning: Recent Progress and Prospects,'' \textit{arXiv preprint arXiv:2303.07883}, 2023.

    \bibitem{luo2023multi} Z. Luo, Z. Chen, J. Welsh, ``Multi-agent Reinforcement Learning with Deep Networks for Diverse Q-Vectors,'' \textit{IEEE Transactions on Robotics}, vol. 39, no. 2, 2023.
    
    \bibitem{vinyals2019alphastar} O. Vinyals et al., ``Grandmaster level in StarCraft II using multi-agent reinforcement learning,'' \textit{Nature}, vol. 575, no. 7782, pp. 350–354, 2019.
    
    \bibitem{wu2023minizero} T.-R. Wu et al., ``MiniZero: Comparative Analysis of AlphaZero and MuZero in Go, Othello, and Atari Games,'' \textit{arXiv preprint arXiv:2304.06826}, 2023.
    
    \bibitem{danihelka2022policy} I. Danihelka et al., ``Policy improvement by planning with gumbel,'' in \textit{International Conference on Learning Representations (ICLR)}, 2022.
    
    \bibitem{oikarinen2023robust} T. Oikarinen et al., ``Robust Reinforcement Learning via Adversarial Kernel Approximation,'' in \textit{International Conference on Learning Representations (ICLR)}, 2023.
\bibliographystyle{plain}
\bibliography{references}

\end{thebibliography}
\vspace{12pt}

\end{document}
