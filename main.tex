\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Reinforcement Learning in Strategy-Based and Atari Games: A Review of Google DeepMind's Innovations\\}

\input{sections/0Authors/authors.tex}

\maketitle

\begin{abstract}

    \input{sections/1Abstract/abstract.tex}

\end{abstract}

\begin{IEEEkeywords}
    Deep Reinforcement Learning, Google DeepMind, AlphaGo, AlphaGo Zero, MuZero, Atari Games, Go, Chess, Shogi,
\end{IEEEkeywords}

\section{Introduction}
\input{sections/2Introduction/introdcution.tex}

\section{Background}
\input{sections/3Background/background.tex}

\section{AlphaGo}
\input{sections/4AlphaGo/AlphaGo.tex}

\section{AlphaGo Zero}
\input{sections/5AlphaGo Zero/AlphaGoZero.tex}

\section{MuZero}
\input{sections/6MuZero/MuZero.tex}

\section{Advancements}
\input{sections/7Advancments/advancments.tex}

\section{Challenges and Future Directions}
\input{sections/8Challenges and Future Directions/challengsAndDirections.tex}

\section{Conclusion}
\input{sections/9Conclusion/conclusion.tex}

\section*{Acknowledgment}

\section*{References}

Please number citations consecutively within brackets \cite{firstbib}. The
sentence punctuation follows the bracket \cite{b2}. Refer simply to the
reference number, as in \cite{b3}---do not use ``Ref. \cite{b3}'' or
``reference \cite{b3}'' except at the beginning of a sentence: ``Reference
\cite{b3} was the first $\ldots$''

Number footnotes separately in superscripts. Place the actual footnote at the
bottom of the column in which it was cited. Do not put footnotes in the
abstract or reference list. Use letters for table footnotes.

Unless there are six authors or more give all authors' names; do not use ``et
al.''. Papers that have not been published, even if they have been submitted
for publication, should be cited as ``unpublished'' \cite{b4}. Papers that have
been accepted for publication should be cited as ``in press'' \cite{b5}.
Capitalize only the first word in a paper title, except for proper nouns and
element symbols.

For papers published in translation journals, please give the English citation
first, followed by the original foreign-language citation \cite{b6}.

\begin{thebibliography}{00}
    
    
    
    %-------Background-------
    \bibitem{I1}  N. Y. Georgios and T. Julian, Artificial Intelligence and Games. New York: Springer, 2018.
    \bibitem{I2}  N. Justesen, P. Bontrager, J. Togelius, S. Risi, (2019). Deep learning for video game playing. arXiv. https://doi.org/10.48550/arXiv.1708.07902
    \bibitem{I3}  V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wierstra, M. Riedmiller, (2013). Playing Atari with deep reinforcement learning. arXiv. https://doi.org/10.48550/arXiv.1312.5602
    \bibitem{I4}  A. Graves, G. Wayne, I. Danihelka, (2014). Neural Turing Machines. arXiv. https://doi.org/10.48550/arXiv.1410.5401
    \bibitem{I5}  C.J.C.H.Watkins, P. Dayan, Q-learning. Mach Learn 8, 279–292 (1992). https://doi.org/10.1007/BF00992698
    \bibitem{I6}  DeepMind, (2015, February 12), Deep reinforcement learning. https://deepmind.google/discover/blog/deep-reinforcement-learning/
    \bibitem{I7}  T. Schaul, J. Quan, I. Antonoglou, D. Silver, (2015). Prioritized Experience Replay. arXiv. https://doi.org/10.48550/arXiv.1511.05952
    \bibitem{I8}  V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley, D. Silver, K. Kavukcuoglu, (2016). Asynchronous Methods for Deep Reinforcement Learning. arXiv. https://doi.org/10.48550/arXiv.1602.01783
    \bibitem{I9}  A. Kailash, P. D. Marc, B. Miles, and A. B. Anil, (2017). Deep Reinforcement Learning: A Brief Survey. IEEE Signal Processing Magazine, vol. 34, pp. 26–38, 2017. arXiv. https://doi.org/10.48550/arXiv.1708.05866
    \bibitem{I10} D. Zhao,  K. Shao, Y. Zhu, D. Li, Y. Chen, H. Wang, D. Liu, T. Zhou, and C. Wang, “Review of deep reinforcement learning and discussions on the development of computer Go,” Control Theory and Applications, vol. 33, no. 6, pp. 701–717, 2016 arXiv. https://doi.org/10.48550/arXiv.1812.01123
    \bibitem{I11} Z. Tang, K. Shao, D. Zhao, and Y. Zhu, “Recent progress of deep reinforcement learning: from AlphaGo to AlphaGo Zero,” Control Theory and Applications, vol. 34, no. 12, pp. 1529–1546, 2017.
    \bibitem{I12} K. Shao, Z. Tang, Y. Zhu, N. Li, D. Zhao, (2019). A survey of deep reinforcement learning in video games. arXiv. https://arxiv.org/abs/1912.10944
    
    \bibitem{bg1} L. Thorndike and D. Bruce, Animal Intelligence. Routledge, 2017.
    \bibitem{bg2} R. S. Sutton and A. Barto, Reinforcement learning : an introduction. Cambridge, Ma ; London: The Mit Press, 2018.
    \bibitem{bg3} A. Kumar Shakya, G. Pillai, and S. Chakrabarty, “Reinforcement Learning Algorithms: A brief survey,” Expert Systems with Applications, vol. 231, p. 120495, May 2023
    

    % \bibitem{b3} I. S. Jacobs and C. P. Bean, ``Fine particles, thin films and exchange anisotropy,'' in Magnetism, vol. III, G. T. Rado and H. Suhl, Eds. New York: Academic, 1963, pp. 271--350.
    % \bibitem{b4} K. Elissa, ``Title of paper if known,'' unpublished.
    % \bibitem{b5} R. Nicole, ``Title of paper with only first word capitalized,'' J. Name Stand. Abbrev., in press.
    % \bibitem{b6} Y. Yorozu, M. Hirano, K. Oka, and Y. Tagawa, ``Electron spectroscopy studies on magneto-optical media and plastic substrate interface,'' IEEE Transl. J. Magn. Japan, vol. 2, pp. 740--741, August 1987 [Digests 9th Annual Conf. Magnetics Japan, p. 301, 1982].
    % \bibitem{b7} M. Young, The Technical Writer's Handbook. Mill Valley, CA: University Science, 1989.
\end{thebibliography}
\vspace{12pt}

\end{document}