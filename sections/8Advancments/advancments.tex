
\label{sec:applications}

The true value of the deep reinforcement learning paradigm pioneered by the DeepMind models is demonstrated by its successful translation into high-impact real-world applications. These models excel in domains that can be framed as sequential decision-making problems with clear objectives, often discovering novel strategies that surpass human-designed solutions. The following case studies illustrate how algorithms developed for games are now driving innovation across mathematics, computer science, systems engineering, and biology.

\subsection{Matrix Multiplication: Alpha Tensor}
\label{subsec:alphatensor}

A prime example of this translation is \textbf{Alpha Tensor}, a deep RL model that adapts the AlphaZero algorithm to the fundamental problem of discovering efficient algorithms for matrix multiplication \cite{fawzi2022discovering}. Alpha Tensor frames the problem as a single-player game, the \textit{Tensor Game}. The state of the game is a three-dimensional tensor, and actions correspond to updating this tensor. The model is rewarded for finding a path that factorizes the initial matrix multiplication tensor into the zero tensor in the fewest steps, corresponding to the lowest \textit{rank} of the multiplication algorithm.

Searching a space of predefined factor entries, the model discovers the optimal multiplication algorithm from scratch. Through this process, Alpha Tensor discovered algorithms that matched or surpassed the best human-developed ones. A notable achievement was for $4 \times 4$ matrices, where it discovered a rank-47 algorithm, an improvement over the long-standing best-known rank-49 (Strassen's) algorithm. Beyond finding a single optimal solution, Alpha Tensor generated a vast database of distinct, non-equivalent multiplication algorithms, providing a valuable resource for further mathematical research. Furthermore, the model's reward function can be adjusted to optimize for specific hardware capabilities, demonstrating its flexibility in generating hardware-aware algorithms that minimize latency or energy consumption.

\subsection{Sorting Algorithms: AlphaDev}
\label{subsec:alphadev}

Sorting is one of the most common subroutines in programming, and as the demand for computation increases, the use of such fundamental algorithms grows. As such, optimizing them is critical for computational efficiency. \textbf{AlphaDev} adapts the AlphaZero algorithm to this task by modeling the sequence of low-level CPU instructions (assembly code) for a sorting function as a single-player game, the \textit{AssemblyGame} \cite{alphadev2023}. The goal of the game is to find a correct program that minimizes latency (a lower ``score'' is better).

The model searches the game space for the shortest, fastest, and correct algorithm. For fixed-length sorting (e.g., sorting lists of length 3, 4, and 5), it discovered new algorithms that outperformed the best-known human-designed solutions in the standard C++ library. Even for the more complex variable-length sort, AlphaDev generated enhancements. Notably, the improved sorting libraries discovered by AlphaDev have been integrated into the standard C++ \texttt{libc++} library, providing efficiency gains for millions of developers and applications worldwide.

\subsection{Compression Optimization: MuZero RC}
\label{subsec:muzerorc}

A notable implementation of MuZero has been in collaboration with YouTube, where it was used to optimize video compression within the open-source VP9 codec. This implementation, called \textbf{MuZero Rate-Controller (MuZero-RC)}, adapted MuZero's ability to predict and plan to the complex, practical task of video streaming \cite{muzero_real_world_2022}.

By optimizing the encoding process, MuZero-RC achieved an average bitrate reduction of 4\% without degrading video quality. This improvement directly impacts the efficiency of video streaming services such as YouTube, leading to faster loading times and reduced data usage for users. This application exemplifies how the model-based reinforcement learning principles behind MuZero can address practical real-world challenges outside of games, making computer systems more efficient and less resource-intensive.

\subsection{Protein Folding: AlphaFold}
\label{subsec:alphafold}

\textbf{AlphaFold} addresses one of the most challenging problems in biology: predicting the three-dimensional structure of a protein from its amino acid sequence \cite{jumper2021highly}. While primarily a feat of supervised learning, AlphaFold's training was refined using reinforcement learning to improve the accuracy of its predicted protein structures. The model operates on a feedback loop where it is rewarded for generating structures that match known experimental data.

This process of iterative refinement, guided by a reward signal, aligns with the core RL principles explored in this paper. The architecture of AlphaFold includes deep neural networks that analyze both the sequential and spatial relationships between amino acids. By training on extensive datasets of known protein structures, AlphaFold has achieved unprecedented accuracy, often rivaling experimental methods such as X-ray crystallography. This breakthrough has had a transformative impact on biological research and drug discovery.
